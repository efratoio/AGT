%!TEX root = main.tex
\subsection{Approximation Algorithm for the Knapsack Problem}\label{sec:knapsack}
In this section we will discuss the approximation algorithm presented in [second paper] for an adaptive policy that gives a constant factor $ (\approx 0.0259)$ approximation for the optimal solution that can be found in polynomial time. The section is composed from two parts: in the first part, we will present an optimal non-adaptive approximation algorithm (which is also relaxed and randomized) and in the second part we will present an approximation algorithm for the optimal adaptive policy, which uses the non-adaptive algorithm.
\subsubsection{An Optimal Non-Adaptive Approximation Algorithm}
In this algorithm we will use the notion of \textit{densities} and \textit{marginal density}, therefore, we will begin with some definitions:
\\The \textit{marginal value} of some policy $(H, r)$ to an existing policy $(S, q)$ is denoted by $F_{(S,q)}(H, r)$ and defined as $F_{(S,q)}(H, r) = F(S \cup H, q \vee r) − F(S, q)$. Similarly, the marginal cost is $C_{(S,q)}(H, r) = C(S \cup H, q \vee r) − C(S, q)$. The marginal density of a policy is given by: 
\[
    D_{S,q}(H, r) = 
\begin{cases}
    \frac{F_{S,q}(H, r)}{C_{S,q}(H, r)}& \text{if } (S, q) \neq (H, r)\\
    0              & \text{otherwise}
\end{cases}
\]

Finally, $w$ denotes the expected costs vector, defined as the element-wise multiplication of c and p (i.e. the costs and probabilities of the model).

The algorithm considers groups of $\frac{1}{\epsilon}$ nodes from $X$ and greedily adds $x \in X$ to $S$ as long as there is enough budget left while updating $q$ in a way that maximizes the \textbf{density} of $(S,q)$. The update of the probabilities vector $q$ of $N(X)$ finds an approximately densent addition of first-stage node $x \in X$.  For each $x$, it greedily adds the second-stage neighbors of $x$ that maximize the marginal density, until either the budget is exhausted or until the marginal density can no longer be improved.  In the end it chooses the completed solution which has the maximum value (out of the groups of size $\frac{1}{\epsilon}$ it began with). pseudo-code of the algorithm can be found in [???add pics?] divided to the main algorithm \textbf{DensestAscent} and a procedure \textbf{AddDensest}. 
\begin{figure}[h]
\includegraphics[width=8cm]{KSDA.PNG}
\centering
\end{figure}
\begin{figure}[h]
\includegraphics[width=8cm]{KSAD.PNG}
\centering
\end{figure}
It can be proven that \textbf{AddDensest} returns a $(1 - 1/e)$-approximation to the maximal marginal density. This approximation can be translated to a $(1 - 1/e^{1 - 1/e} - \epsilon)$-approximation guarantee for \textbf{DensestAscent} and overcoming the knapsack constrains complication by considering the fact that we tested all subsets $S \subseteq X$ of size $1/\epsilon$. It can be shown that for an optimal solution $(O, v)$ \textbf{DensestAscent} returns $(S, q)$ such that $F(S, q) \ge (1 − 1/e^{1−1/e})(1 − \epsilon)F(O, v)$ and by that proving the following theorem: 
\begin{theorem} For any constant $\epsilon > 0$, \textbf{DensestAscent} is a $(1 - 1/e^{1 - 1/e} - O(\epsilon))$-approximation algorithm of the optimal non-adaptive policy for general submodular functions.
\end{theorem}

\subsubsection{Adaptive Policies}
In this section we will present an adaptive policy that is based on the algorithm from the previous section. There are two main points that need to be considered when using a relaxed non adaptive policy as an adaptive one. The first one is that we need to show that a non-adaptive policy (relaxed and randomized) is a good approximation to an adaptive policy. This we can get as a consequence from a Lemma in [A. Badanidiyuru, C. Papadimitriou, A. Rubinstein, L. Seeman, and Y. Singer. Submodular
adaptive seeding. Manuscript, 2015.]. The lemma proves that the gap is $(1 - 1/e)$, and that it is tight. The second point to be considered is that the objective value we get from the nan-adaptive policy can be approximately obtain over the realizations of the nodes in the second stage. This point is harder to overcome as we need to divide the budget between the first and second stages apriori before the nodes in the second stage are being realized. We will handle it by first solving it with a apecial case, in which the cost of each node is not too high.
\textbf{small costs}
In this part we assume that that no node as a cost which is a constant ratio of the total budget. This is a reasonable assumption, as while different nodes can have much different costs, it is very unlikely that a single node will cost a fraction of the total budget. This assumtion makes it possible to convert non-adaptive policies into adaptive ones, with a small loss. We will prove this by using two lemmas (given without proves, proves can be found in[]):
lemma 5.3, lemma 5.4.
present theorem 5.2 and prove it.
\textbf{arbitrarily high costs}
The main idea for overcoming the obstacle of the two stages nature of the problem with adaptive policy that is based on a non-adaptive policy is to consider the optimal adaptive policy as several restricted policies, where each one of these restrictions can be approximated either by using the non-adaptive relaxation, or through an adaptive policy which can be found using sampling and standard techniques for submodular maximization.



the two points needed to be considered
small costs: two lemmas without proofs, theorem 5.2 and proof
general case: introduction, nice, opttc, opttp, all three 